{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-12T00:04:09.115553Z",
     "start_time": "2025-06-12T00:04:09.110755Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 315
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Dataset preparation",
   "id": "28ebae11ef71ed81"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T00:04:09.167907Z",
     "start_time": "2025-06-12T00:04:09.163061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Pre-processes dataset and splits it into train and test-set in 7:3 ratio\n",
    "def preprocessing(df, y_label, frac):\n",
    "\n",
    "    #Drop rows with missing values (if any)\n",
    "    df = df.dropna()\n",
    "\n",
    "    #split into train and test set\n",
    "    df_train = df.sample(frac=frac, axis=0)\n",
    "    df_test = df.drop(df_train.index)\n",
    "\n",
    "    #Choose target feature\n",
    "    y_train = df_train[y_label].to_numpy()\n",
    "    X_train = df_train.drop(columns=y_label).to_numpy()\n",
    "\n",
    "    y_test = df_test[y_label].to_numpy()\n",
    "    X_test = df_test.drop(columns=y_label).to_numpy()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n"
   ],
   "id": "4a84ee45fcbc8cf3",
   "outputs": [],
   "execution_count": 316
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T00:04:09.223216Z",
     "start_time": "2025-06-12T00:04:09.214181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Car evaluation (https://archive.ics.uci.edu/ml/datasets/Car+Evaluation) Target attribute: safety (low, med, high).\n",
    "df_car = pd.read_csv('car+evaluation/car.data', sep=',', header=None)\n",
    "Xtrain_car, ytrain_car, Xtest_car, ytest_car = preprocessing(df_car, 5, 0.7)\n"
   ],
   "id": "5550343dee3a1cd7",
   "outputs": [],
   "execution_count": 317
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Decision Tree Implementation",
   "id": "6192ce692a3396de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T00:04:09.284802Z",
     "start_time": "2025-06-12T00:04:09.263319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Decision tree implementation inspired from: https://medium.com/@omidsaghatchian/decision-tree-implementation-from-scratch-visualization-5eb0bbf427c2\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data: numpy.ndarray, default=None\n",
    "        The dataset includes X and Y\n",
    "    children: dict(feat_value: Node), default=None\n",
    "        Dict of children\n",
    "    split_on: int, default=None\n",
    "        Index of the feature that node was split on that\n",
    "    pred_class : str, default=None\n",
    "        The predicted class for the node (only applicable to leaf nodes)\n",
    "    is_leaf: bool, default=False\n",
    "        Determine whether the node is leaf or not\n",
    "    gain: float, default=None\n",
    "        Information gained by performing split\n",
    "    class_prob: dict(feat_value: class_prob), default=None\n",
    "        dict of class probabilities\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self ,data=None, children=None, split_on = None, pred_class=None, is_leaf=False, gain=None, class_prob=None, entropy=None, gini=None):\n",
    "\n",
    "        self.data = data\n",
    "        self.children = children\n",
    "        self.split_on = split_on\n",
    "        self.pred_class = pred_class\n",
    "        self.is_leaf = is_leaf\n",
    "        self.gain = gain\n",
    "        self.class_prob = class_prob\n",
    "        self.entropy = entropy\n",
    "        self.gini = gini\n",
    "\n",
    "\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "\n",
    "    def __init__(self, criterium, max_depth):\n",
    "        self.root = Node()\n",
    "        self.max_depth = max_depth\n",
    "        #Impurity criterium used for splitting nodes\n",
    "        self.criterium = criterium\n",
    "\n",
    "\n",
    "    def calculate_entropy(self, y):\n",
    "        \"\"\"\n",
    "        Calculates entropy of the given label array\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y: numpy.ndarray\n",
    "            The labels array.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        entropy: flaot\n",
    "            The entropy value of the given labels.\n",
    "\n",
    "        \"\"\"\n",
    "        _, labels_counts = np.unique(y, return_counts=True)\n",
    "        total_instances = len(y)\n",
    "        entropy = sum([label_count / total_instances * np.log2(1 / (label_count / total_instances)) for label_count in labels_counts])\n",
    "        return entropy\n",
    "\n",
    "\n",
    "    def calculate_gini(self, y):\n",
    "        \"\"\"\n",
    "        Calculate Gini index for array y of classes.\n",
    "        \"\"\"\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        p = counts / counts.sum()\n",
    "        gini = 1 - np.sum(p ** 2)\n",
    "        return gini\n",
    "\n",
    "\n",
    "    def calculate_class_probabilities(self, y):\n",
    "        counts = Counter(y)\n",
    "        total = len(y)\n",
    "        return {cls: count / total for cls, count in counts.items()}\n",
    "\n",
    "\n",
    "    def split_on_feature(self, data, feat_index, criterium):\n",
    "        \"\"\"\n",
    "        Split the dataset based on a specific feature index.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data: numpy.ndarray\n",
    "            The dataset to be split.\n",
    "\n",
    "        feat_index: int\n",
    "            The index of the feature to perform the split.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        - split_nodes: dict\n",
    "            A dictionary of split nodes.\n",
    "            (feature value as key, corresponding node as value)\n",
    "\n",
    "        - weighted_entropy: float\n",
    "            The weighted entropy of the split.\n",
    "        \"\"\"\n",
    "        feature_values = data[:, feat_index]\n",
    "        unique_values = np.unique(feature_values)\n",
    "\n",
    "        split_nodes = {}\n",
    "        weighted_impurity = 0\n",
    "        total_instances = len(data)\n",
    "\n",
    "        for unique_value in unique_values:\n",
    "            partition = data[data[:, feat_index] == unique_value, :]\n",
    "            node = Node(data=partition)\n",
    "            split_nodes[unique_value] = node\n",
    "            partition_y = partition[:, -1]\n",
    "            #iteratively calculate entropy for each child node and use it to compute weighted entropy of the split\n",
    "            if criterium == 'entropy':\n",
    "                node_entropy = self.calculate_entropy(partition_y)\n",
    "                weighted_impurity += (len(partition) / total_instances) * node_entropy\n",
    "            else:\n",
    "                node_gini = self.calculate_gini(partition_y)\n",
    "                weighted_impurity += (len(partition) / total_instances) * node_gini\n",
    "\n",
    "        return split_nodes, weighted_impurity\n",
    "\n",
    "\n",
    "    def best_split(self, node, criterium, depth=0):\n",
    "        \"\"\"\n",
    "        Find the best split for the given node.\n",
    "        (data in node.data)\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        node: Node\n",
    "            The node for which the best split is being determined.\n",
    "\n",
    "        If the node meets the criteria to stop splitting:\n",
    "            - Mark the node as a leaf.\n",
    "            - Assign a predicted class for future predictions based on the target values (y).\n",
    "            - return.\n",
    "\n",
    "        Otherwise:\n",
    "            - Initialize variables for tracking the best split.\n",
    "            - Iterate over the features to find the best split.\n",
    "            - Split the data based on each feature and calculate the weighted entropy of the split.\n",
    "            - Recursively call the best_split function for each child node.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y = self.get_y(node.data)\n",
    "        #Impurity of current node\n",
    "        if criterium == 'entropy':\n",
    "            node_entropy = self.calculate_entropy(y)\n",
    "            node.entropy = node_entropy\n",
    "        else:\n",
    "            node_gini = self.calculate_gini(y)\n",
    "            node.gini = node_gini\n",
    "        #Class probabilities\n",
    "        node.class_prob = self.calculate_class_probabilities(y)\n",
    "\n",
    "\n",
    "        # Check if the node meets the criteria to stop splitting\n",
    "        if self.meet_criteria(node, depth):\n",
    "            node.is_leaf = True\n",
    "            y = self.get_y(node.data)\n",
    "            node.pred_class = self.get_pred_class(y)\n",
    "            return\n",
    "\n",
    "        # Initialize variables for tracking the best split\n",
    "        feature_split_index = -1\n",
    "        min_impurity = float('inf')\n",
    "        child_nodes = None\n",
    "\n",
    "\n",
    "        # iterate over all features, ignore (y)\n",
    "        for i in range(node.data.shape[1] - 1):\n",
    "            split_nodes, weighted_impurity = self.split_on_feature(node.data, i, criterium)\n",
    "            if weighted_impurity < min_impurity:\n",
    "                child_nodes, min_impurity = split_nodes, weighted_impurity\n",
    "                feature_split_index = i\n",
    "\n",
    "\n",
    "\n",
    "        node.children = child_nodes\n",
    "        node.split_on = feature_split_index\n",
    "\n",
    "        #calculate information gain gained by splitting\n",
    "        if criterium == 'entropy':\n",
    "            node.gain = node_entropy -  min_impurity\n",
    "        else:\n",
    "            node.gain = node_gini - min_impurity\n",
    "\n",
    "\n",
    "        # Recursively call the best_split function for each child node\n",
    "        for child_node in child_nodes.values():\n",
    "            self.best_split(child_node, criterium, depth + 1)\n",
    "\n",
    "\n",
    "    #Stopping criteria\n",
    "    def meet_criteria(self, node, depth):\n",
    "        \"\"\"\n",
    "        Check if maximum depth has been reached or entropy calculated is 0\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        node : Node\n",
    "            The node to check for meeting the stopping criteria.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        bool\n",
    "            True if the criteria is met, False otherwise.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        y = self.get_y(node.data)\n",
    "        return True if self.calculate_entropy(y) == 0 or depth >= self.max_depth else False\n",
    "\n",
    "\n",
    "    def get_y(self, data):\n",
    "        \"\"\"\n",
    "        Get the target (y) from the data.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : numpy.ndarray\n",
    "            The input data containing features and the target variable.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        y: numpy.ndarray\n",
    "            The target variable extracted from the data.\n",
    "\n",
    "        \"\"\"\n",
    "        y = data[:, -1]\n",
    "        return y\n",
    "\n",
    "\n",
    "    def get_pred_class(self, y):\n",
    "        \"\"\"\n",
    "        Get the predicted class label based on the majority vote.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y : numpy.ndarray\n",
    "            The array of class labels.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        str\n",
    "            The predicted class label.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        labels, labels_counts = np.unique(y, return_counts=True)\n",
    "        index = np.argmax(labels_counts)\n",
    "        return labels[index]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the decision tree model to the provided dataset.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: numpy.ndarray\n",
    "            The input features of the dataset.\n",
    "\n",
    "        y: numpy.ndarray\n",
    "            The target labels of the dataset.\n",
    "        \"\"\"\n",
    "        data = np.column_stack([X, y])\n",
    "        self.root.data = data\n",
    "        self.best_split(self.root, self.criterium, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the given input features.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: numpy.ndarray\n",
    "            The input features for which to make predictions. Should be a 2D array-like object.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        predictions: numpy.ndarray\n",
    "            An array of predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Traverse the decision tree for each input and make predictions\n",
    "        predictions = np.array([self.traverse_tree(x, self.root) for x in X])\n",
    "        return predictions\n",
    "\n",
    "    def traverse_tree(self, x, node):\n",
    "        \"\"\"\n",
    "        Recursively traverse the decision tree to predict the class label for a given input.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x:\n",
    "            The input for which to make a prediction.\n",
    "\n",
    "        node:\n",
    "            The current node being traversed in the decision tree.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        predicted_class:\n",
    "            The predicted class label for the input feature.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if the current node is a leaf node\n",
    "        if node.is_leaf:\n",
    "            return node.pred_class\n",
    "\n",
    "        # Get the feature value at the split point for the current node\n",
    "        feat_value = x[node.split_on]\n",
    "\n",
    "         # Handle unseen feature values\n",
    "        if feat_value not in node.children:\n",
    "            return node.pred_class  # fallback to majority class at this node\n",
    "\n",
    "        # Recursively traverse the decision tree using the child node corresponding to the feature value\n",
    "        predicted_class = self.traverse_tree(x, node.children[feat_value])\n",
    "\n",
    "        return predicted_class\n",
    "\n",
    "\n",
    "    #Pretty prints tree using ASCII characters. Each node displays Impurity, gain, depth, class probabilities and feature on which the node was split\n",
    "    def print_tree(self, node=None, prefix=\"\", is_last=True, depth=0):\n",
    "        if node is None:\n",
    "            node = self.root\n",
    "\n",
    "        connector = \"└── \" if is_last else \"├── \"\n",
    "        print(prefix + connector, end=\"\")\n",
    "\n",
    "        # Format class probabilities string if available\n",
    "        if node.class_prob:\n",
    "            probs_str = \", \".join([f\"{cls}: {prob:.2f}\" for cls, prob in node.class_prob.items()])\n",
    "        else:\n",
    "            probs_str = \"No class probabilities\"\n",
    "\n",
    "        # Choose correct impurity label\n",
    "        if node.entropy is not None:\n",
    "            impurity_str = f\"Entropy: {node.entropy:.4f}\"\n",
    "        elif node.gini is not None:\n",
    "            impurity_str = f\"Gini: {node.gini:.4f}\"\n",
    "        else:\n",
    "            impurity_str = \"Impurity: N/A\"\n",
    "\n",
    "        # Leaf or internal node output\n",
    "        if node.is_leaf:\n",
    "            print(f\"[Leaf] Depth: {depth}, Class: {node.pred_class}, {impurity_str}, Probs: {{{probs_str}}}\")\n",
    "        else:\n",
    "            print(f\"X[{node.split_on}], Gain: {node.gain:.4f}, Depth: {depth}, {impurity_str}, Probs: {{{probs_str}}}\")\n",
    "\n",
    "        # Recursively print children\n",
    "        if node.children:\n",
    "            prefix += \"    \" if is_last else \"│   \"\n",
    "            child_count = len(node.children)\n",
    "            for i, (feat_val, child) in enumerate(node.children.items()):\n",
    "                is_last_child = (i == child_count - 1)\n",
    "                print(prefix + (\"└── \" if is_last_child else \"├── \") + f\"X[{node.split_on}] = {feat_val}\")\n",
    "                self.print_tree(child, prefix + (\"    \" if is_last_child else \"│   \"), True, depth + 1)\n",
    "\n",
    "    def predict_probs(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities for each instance in X.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: numpy.ndarray\n",
    "            Feature matrix (num_samples, num_features)\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        probs: List[Dict]\n",
    "            A list where each item is a dictionary mapping class labels to probabilities.\n",
    "        \"\"\"\n",
    "        probs = []\n",
    "        for instance in X:\n",
    "            node = self.root\n",
    "            while not node.is_leaf:\n",
    "                feat_val = instance[node.split_on]\n",
    "                if feat_val in node.children:\n",
    "                    node = node.children[feat_val]\n",
    "                else:\n",
    "                    # If feature value not seen in training, break and use current node\n",
    "                    break\n",
    "            probs.append(node.class_prob)\n",
    "        return probs\n",
    "\n",
    "\n",
    "#Using class level labels to calculate cross entropy loss\n",
    "def cross_entropy_loss(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    loss = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        p = 1 - eps if yt == yp else eps\n",
    "        loss += -np.log(p)\n",
    "    return loss / len(y_true)\n",
    "\n",
    "#Cross entropy calculated using predicted probabilities\n",
    "def true_cross_entropy_loss(y_true, y_pred_probs, eps=1e-9):\n",
    "    loss = 0\n",
    "    for true, probs_dict in zip(y_true, y_pred_probs):\n",
    "        p = probs_dict.get(true, 0)\n",
    "        loss += -np.log(p + eps)\n",
    "    return loss / len(y_true)\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(np.array(y_true) == np.array(y_pred))\n"
   ],
   "id": "a37437d77e2f7d1a",
   "outputs": [],
   "execution_count": 318
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Instead of printing histograms for each node, for the sake of readability we print the class probabilities and impurity measure in the tree within the node itself.",
   "id": "a807c17481fd49f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We start by building and printing the decision tree for the car dataset using Cross entropy as the quality measure.",
   "id": "e598d3ba37351754"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T00:04:09.362232Z",
     "start_time": "2025-06-12T00:04:09.331962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = DecisionTreeClassifier(\"entropy\", max_depth=3)\n",
    "model.fit(Xtrain_car, ytrain_car)\n",
    "\n",
    "print(\"ROOT\")\n",
    "model.print_tree(model.root)\n",
    "\n",
    "#predicting labels using the test set\n",
    "y_pred = model.predict(Xtest_car)\n",
    "\n",
    "loss = cross_entropy_loss(ytest_car, y_pred)\n",
    "true_loss = true_cross_entropy_loss(ytest_car, model.predict_probs(Xtest_car))\n",
    "acc = accuracy(ytest_car, y_pred)\n",
    "\n",
    "print(f\"\\nCross Entropy Loss using Hard Labels: {loss:.4f}\")\n",
    "print(f\"Cross Entropy Loss using Probabilities: {true_loss:.4f}\")\n",
    "print(f\"Accuracy on Test-Set: {acc * 100:.2f}%\")\n"
   ],
   "id": "d75b73d9944f86da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT\n",
      "└── X[5], Gain: 0.2731, Depth: 0, Entropy: 1.5846, Probs: {low: 0.34, high: 0.33, med: 0.32}\n",
      "    ├── X[5] = acc\n",
      "    │   └── X[0], Gain: 0.0352, Depth: 1, Entropy: 0.9806, Probs: {high: 0.58, med: 0.42}\n",
      "    │       ├── X[0] = high\n",
      "    │       │   └── X[4], Gain: 0.1530, Depth: 2, Entropy: 0.8941, Probs: {high: 0.69, med: 0.31}\n",
      "    │       │       ├── X[4] = big\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.9940, Probs: {med: 0.45, high: 0.55}\n",
      "    │       │       ├── X[4] = med\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.9183, Probs: {high: 0.67, med: 0.33}\n",
      "    │       │       └── X[4] = small\n",
      "    │       │           └── [Leaf] Depth: 3, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │       ├── X[0] = low\n",
      "    │       │   └── X[1], Gain: 0.3027, Depth: 2, Entropy: 0.9880, Probs: {med: 0.56, high: 0.44}\n",
      "    │       │       ├── X[1] = high\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Entropy: 0.9044, Probs: {med: 0.68, high: 0.32}\n",
      "    │       │       ├── X[1] = low\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │       │       ├── X[1] = med\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │       │       └── X[1] = vhigh\n",
      "    │       │           └── [Leaf] Depth: 3, Class: high, Entropy: 0.7950, Probs: {med: 0.24, high: 0.76}\n",
      "    │       ├── X[0] = med\n",
      "    │       │   └── X[1], Gain: 0.1666, Depth: 2, Entropy: 0.9988, Probs: {med: 0.48, high: 0.52}\n",
      "    │       │       ├── X[1] = high\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.8555, Probs: {high: 0.72, med: 0.28}\n",
      "    │       │       ├── X[1] = low\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │       │       ├── X[1] = med\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Entropy: 0.8631, Probs: {high: 0.29, med: 0.71}\n",
      "    │       │       └── X[1] = vhigh\n",
      "    │       │           └── [Leaf] Depth: 3, Class: high, Entropy: 0.9544, Probs: {med: 0.38, high: 0.62}\n",
      "    │       └── X[0] = vhigh\n",
      "    │           └── X[4], Gain: 0.1438, Depth: 2, Entropy: 0.8905, Probs: {med: 0.31, high: 0.69}\n",
      "    │               ├── X[4] = big\n",
      "    │               │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.9656, Probs: {med: 0.39, high: 0.61}\n",
      "    │               ├── X[4] = med\n",
      "    │               │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.9774, Probs: {med: 0.41, high: 0.59}\n",
      "    │               └── X[4] = small\n",
      "    │                   └── [Leaf] Depth: 3, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    ├── X[5] = good\n",
      "    │   └── X[4], Gain: 0.6222, Depth: 1, Entropy: 0.9954, Probs: {high: 0.46, med: 0.54}\n",
      "    │       ├── X[4] = big\n",
      "    │       │   └── [Leaf] Depth: 2, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │       ├── X[4] = med\n",
      "    │       │   └── X[2], Gain: 0.7264, Depth: 2, Entropy: 0.9819, Probs: {high: 0.42, med: 0.58}\n",
      "    │       │       ├── X[2] = 2\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │       │       ├── X[2] = 3\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.9710, Probs: {high: 0.60, med: 0.40}\n",
      "    │       │       ├── X[2] = 4\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │       │       └── X[2] = 5more\n",
      "    │       │           └── [Leaf] Depth: 3, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │       └── X[4] = small\n",
      "    │           └── [Leaf] Depth: 2, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    ├── X[5] = unacc\n",
      "    │   └── X[3], Gain: 0.0575, Depth: 1, Entropy: 1.5032, Probs: {low: 0.49, high: 0.22, med: 0.30}\n",
      "    │       ├── X[3] = 2\n",
      "    │       │   └── X[2], Gain: 0.0064, Depth: 2, Entropy: 1.5832, Probs: {high: 0.32, med: 0.33, low: 0.35}\n",
      "    │       │       ├── X[2] = 2\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: low, Entropy: 1.5705, Probs: {med: 0.36, high: 0.27, low: 0.37}\n",
      "    │       │       ├── X[2] = 3\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Entropy: 1.5781, Probs: {med: 0.33, low: 0.30, high: 0.38}\n",
      "    │       │       ├── X[2] = 4\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: low, Entropy: 1.5790, Probs: {med: 0.32, low: 0.38, high: 0.31}\n",
      "    │       │       └── X[2] = 5more\n",
      "    │       │           └── [Leaf] Depth: 3, Class: low, Entropy: 1.5793, Probs: {high: 0.31, med: 0.32, low: 0.38}\n",
      "    │       ├── X[3] = 4\n",
      "    │       │   └── X[0], Gain: 0.1420, Depth: 2, Entropy: 1.2862, Probs: {low: 0.62, med: 0.27, high: 0.11}\n",
      "    │       │       ├── X[0] = high\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: low, Entropy: 1.3627, Probs: {low: 0.55, med: 0.33, high: 0.12}\n",
      "    │       │       ├── X[0] = low\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: low, Entropy: 0.5665, Probs: {low: 0.87, med: 0.13}\n",
      "    │       │       ├── X[0] = med\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: low, Entropy: 0.7593, Probs: {low: 0.78, med: 0.22}\n",
      "    │       │       └── X[0] = vhigh\n",
      "    │       │           └── [Leaf] Depth: 3, Class: low, Entropy: 1.5487, Probs: {med: 0.33, high: 0.24, low: 0.42}\n",
      "    │       └── X[3] = more\n",
      "    │           └── X[0], Gain: 0.1093, Depth: 2, Entropy: 1.3568, Probs: {low: 0.59, med: 0.27, high: 0.14}\n",
      "    │               ├── X[0] = high\n",
      "    │               │   └── [Leaf] Depth: 3, Class: low, Entropy: 1.4401, Probs: {low: 0.47, med: 0.39, high: 0.14}\n",
      "    │               ├── X[0] = low\n",
      "    │               │   └── [Leaf] Depth: 3, Class: low, Entropy: 0.8567, Probs: {low: 0.82, high: 0.06, med: 0.12}\n",
      "    │               ├── X[0] = med\n",
      "    │               │   └── [Leaf] Depth: 3, Class: low, Entropy: 0.9246, Probs: {low: 0.78, high: 0.04, med: 0.18}\n",
      "    │               └── X[0] = vhigh\n",
      "    │                   └── [Leaf] Depth: 3, Class: low, Entropy: 1.5571, Probs: {low: 0.42, med: 0.32, high: 0.26}\n",
      "    └── X[5] = vgood\n",
      "        └── [Leaf] Depth: 1, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "\n",
      "Cross Entropy Loss using Hard Labels: 10.2016\n",
      "Cross Entropy Loss using Probabilities: 0.8517\n",
      "Accuracy on Test-Set: 50.77%\n"
     ]
    }
   ],
   "execution_count": 319
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next the decision tree is built using the Gini index as the quality criterium",
   "id": "614d1b26c2b31942"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T00:04:09.418013Z",
     "start_time": "2025-06-12T00:04:09.385777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = DecisionTreeClassifier(\"gini\", max_depth=3)\n",
    "model.fit(Xtrain_car, ytrain_car)\n",
    "\n",
    "print(\"ROOT\")\n",
    "model.print_tree(model.root)\n",
    "\n",
    "#predicting labels using the test set\n",
    "y_pred = model.predict(Xtest_car)\n",
    "\n",
    "loss = cross_entropy_loss(ytest_car, y_pred)\n",
    "true_loss = true_cross_entropy_loss(ytest_car, model.predict_probs(Xtest_car))\n",
    "acc = accuracy(ytest_car, y_pred)\n",
    "\n",
    "print(f\"\\nCross Entropy Loss using Hard Labels: {loss:.4f}\")\n",
    "print(f\"Cross Entropy Loss using Probabilities: {true_loss:.4f}\")\n",
    "print(f\"Accuracy on Test-Set: {acc * 100:.2f}%\")\n"
   ],
   "id": "e452dcd46f28ebef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT\n",
      "└── X[5], Gain: 0.0985, Depth: 0, Gini: 0.6665, Probs: {low: 0.34, high: 0.33, med: 0.32}\n",
      "    ├── X[5] = acc\n",
      "    │   └── X[0], Gain: 0.0236, Depth: 1, Gini: 0.4866, Probs: {high: 0.58, med: 0.42}\n",
      "    │       ├── X[0] = high\n",
      "    │       │   └── X[4], Gain: 0.0631, Depth: 2, Gini: 0.4284, Probs: {high: 0.69, med: 0.31}\n",
      "    │       │       ├── X[4] = big\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Gini: 0.4959, Probs: {med: 0.45, high: 0.55}\n",
      "    │       │       ├── X[4] = med\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Gini: 0.4444, Probs: {high: 0.67, med: 0.33}\n",
      "    │       │       └── X[4] = small\n",
      "    │       │           └── [Leaf] Depth: 3, Class: high, Gini: 0.0000, Probs: {high: 1.00}\n",
      "    │       ├── X[0] = low\n",
      "    │       │   └── X[1], Gain: 0.1691, Depth: 2, Gini: 0.4917, Probs: {med: 0.56, high: 0.44}\n",
      "    │       │       ├── X[1] = high\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Gini: 0.4352, Probs: {med: 0.68, high: 0.32}\n",
      "    │       │       ├── X[1] = low\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Gini: 0.0000, Probs: {med: 1.00}\n",
      "    │       │       ├── X[1] = med\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Gini: 0.0000, Probs: {med: 1.00}\n",
      "    │       │       └── X[1] = vhigh\n",
      "    │       │           └── [Leaf] Depth: 3, Class: high, Gini: 0.3648, Probs: {med: 0.24, high: 0.76}\n",
      "    │       ├── X[0] = med\n",
      "    │       │   └── X[1], Gain: 0.1005, Depth: 2, Gini: 0.4992, Probs: {med: 0.48, high: 0.52}\n",
      "    │       │       ├── X[1] = high\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Gini: 0.4032, Probs: {high: 0.72, med: 0.28}\n",
      "    │       │       ├── X[1] = low\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Gini: 0.0000, Probs: {med: 1.00}\n",
      "    │       │       ├── X[1] = med\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Gini: 0.4082, Probs: {high: 0.29, med: 0.71}\n",
      "    │       │       └── X[1] = vhigh\n",
      "    │       │           └── [Leaf] Depth: 3, Class: high, Gini: 0.4688, Probs: {med: 0.38, high: 0.62}\n",
      "    │       └── X[0] = vhigh\n",
      "    │           └── X[4], Gain: 0.0570, Depth: 2, Gini: 0.4260, Probs: {med: 0.31, high: 0.69}\n",
      "    │               ├── X[4] = big\n",
      "    │               │   └── [Leaf] Depth: 3, Class: high, Gini: 0.4764, Probs: {med: 0.39, high: 0.61}\n",
      "    │               ├── X[4] = med\n",
      "    │               │   └── [Leaf] Depth: 3, Class: high, Gini: 0.4844, Probs: {med: 0.41, high: 0.59}\n",
      "    │               └── X[4] = small\n",
      "    │                   └── [Leaf] Depth: 3, Class: high, Gini: 0.0000, Probs: {high: 1.00}\n",
      "    ├── X[5] = good\n",
      "    │   └── X[4], Gain: 0.3115, Depth: 1, Gini: 0.4968, Probs: {high: 0.46, med: 0.54}\n",
      "    │       ├── X[4] = big\n",
      "    │       │   └── [Leaf] Depth: 2, Class: med, Gini: 0.0000, Probs: {med: 1.00}\n",
      "    │       ├── X[4] = med\n",
      "    │       │   └── X[2], Gain: 0.3612, Depth: 2, Gini: 0.4875, Probs: {high: 0.42, med: 0.58}\n",
      "    │       │       ├── X[2] = 2\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Gini: 0.0000, Probs: {high: 1.00}\n",
      "    │       │       ├── X[2] = 3\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Gini: 0.4800, Probs: {high: 0.60, med: 0.40}\n",
      "    │       │       ├── X[2] = 4\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Gini: 0.0000, Probs: {med: 1.00}\n",
      "    │       │       └── X[2] = 5more\n",
      "    │       │           └── [Leaf] Depth: 3, Class: med, Gini: 0.0000, Probs: {med: 1.00}\n",
      "    │       └── X[4] = small\n",
      "    │           └── [Leaf] Depth: 2, Class: high, Gini: 0.0000, Probs: {high: 1.00}\n",
      "    ├── X[5] = unacc\n",
      "    │   └── X[3], Gain: 0.0258, Depth: 1, Gini: 0.6280, Probs: {low: 0.49, high: 0.22, med: 0.30}\n",
      "    │       ├── X[3] = 2\n",
      "    │       │   └── X[2], Gain: 0.0029, Depth: 2, Gini: 0.6659, Probs: {high: 0.32, med: 0.33, low: 0.35}\n",
      "    │       │       ├── X[2] = 2\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: low, Gini: 0.6602, Probs: {med: 0.36, high: 0.27, low: 0.37}\n",
      "    │       │       ├── X[2] = 3\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Gini: 0.6635, Probs: {med: 0.33, low: 0.30, high: 0.38}\n",
      "    │       │       ├── X[2] = 4\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: low, Gini: 0.6639, Probs: {med: 0.32, low: 0.38, high: 0.31}\n",
      "    │       │       └── X[2] = 5more\n",
      "    │       │           └── [Leaf] Depth: 3, Class: low, Gini: 0.6640, Probs: {high: 0.31, med: 0.32, low: 0.38}\n",
      "    │       ├── X[3] = 4\n",
      "    │       │   └── X[0], Gain: 0.0468, Depth: 2, Gini: 0.5302, Probs: {low: 0.62, med: 0.27, high: 0.11}\n",
      "    │       │       ├── X[0] = high\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: low, Gini: 0.5721, Probs: {low: 0.55, med: 0.33, high: 0.12}\n",
      "    │       │       ├── X[0] = low\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: low, Gini: 0.2311, Probs: {low: 0.87, med: 0.13}\n",
      "    │       │       ├── X[0] = med\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: low, Gini: 0.3427, Probs: {low: 0.78, med: 0.22}\n",
      "    │       │       └── X[0] = vhigh\n",
      "    │       │           └── [Leaf] Depth: 3, Class: low, Gini: 0.6501, Probs: {med: 0.33, high: 0.24, low: 0.42}\n",
      "    │       └── X[3] = more\n",
      "    │           └── X[0], Gain: 0.0497, Depth: 2, Gini: 0.5595, Probs: {low: 0.59, med: 0.27, high: 0.14}\n",
      "    │               ├── X[0] = high\n",
      "    │               │   └── [Leaf] Depth: 3, Class: low, Gini: 0.6079, Probs: {low: 0.47, med: 0.39, high: 0.14}\n",
      "    │               ├── X[0] = low\n",
      "    │               │   └── [Leaf] Depth: 3, Class: low, Gini: 0.3149, Probs: {low: 0.82, high: 0.06, med: 0.12}\n",
      "    │               ├── X[0] = med\n",
      "    │               │   └── [Leaf] Depth: 3, Class: low, Gini: 0.3615, Probs: {low: 0.78, high: 0.04, med: 0.18}\n",
      "    │               └── X[0] = vhigh\n",
      "    │                   └── [Leaf] Depth: 3, Class: low, Gini: 0.6536, Probs: {low: 0.42, med: 0.32, high: 0.26}\n",
      "    └── X[5] = vgood\n",
      "        └── [Leaf] Depth: 1, Class: high, Gini: 0.0000, Probs: {high: 1.00}\n",
      "\n",
      "Cross Entropy Loss using Hard Labels: 10.2016\n",
      "Cross Entropy Loss using Probabilities: 0.8517\n",
      "Accuracy on Test-Set: 50.77%\n"
     ]
    }
   ],
   "execution_count": 320
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The loss and accuracy for the trees built using the cross entropy and gini index criteria are identical",
   "id": "5aa1797e426499db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pruning the Decision Tree",
   "id": "6c9568d2b60bd548"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T00:04:09.446432Z",
     "start_time": "2025-06-12T00:04:09.440123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prune(node, X_val, y_val, model):\n",
    "    if node.is_leaf or not node.children:\n",
    "        return\n",
    "\n",
    "    for child in node.children.values():\n",
    "        prune(child, X_val, y_val, model)\n",
    "\n",
    "    #calculate accuracy of tree pre-pruning current node\n",
    "    val_pred_original = model.predict(X_val)\n",
    "    acc_original = accuracy(y_val, val_pred_original)\n",
    "\n",
    "    # Save state\n",
    "    original_children = node.children\n",
    "    original_is_leaf = node.is_leaf\n",
    "    original_pred = node.pred_class\n",
    "\n",
    "    # Try pruning\n",
    "    node.children = None\n",
    "    node.is_leaf = True\n",
    "    node.pred_class = model.get_pred_class(model.get_y(node.data))\n",
    "\n",
    "    # Evaluate performance on validation set after pruning\n",
    "    val_pred_pruned = model.predict(X_val)\n",
    "    acc_pruned = accuracy(y_val, val_pred_pruned)\n",
    "\n",
    "\n",
    "    # Restore original tree if accuracy better before pruning\n",
    "    if acc_pruned < acc_original:\n",
    "        node.children = original_children\n",
    "        node.is_leaf = original_is_leaf\n",
    "        node.pred_class = original_pred\n",
    "\n",
    "#Split data into train, test and validation set in the ratio of 34:33:33\n",
    "def train_test_val(df, y_label):\n",
    "\n",
    "    #Drop rows with missing values (if any)\n",
    "    df = df.dropna()\n",
    "\n",
    "    #split into train and temp set\n",
    "    df_train = df.sample(frac=0.34, axis=0)\n",
    "    df_temp = df.drop(df_train.index)\n",
    "\n",
    "    df_val = df_temp.sample(frac=0.5, axis=0)\n",
    "    df_test = df_temp.drop(df_val.index)\n",
    "\n",
    "    #Choose target feature\n",
    "    y_train = df_train[y_label].to_numpy()\n",
    "    X_train = df_train.drop(columns=y_label).to_numpy()\n",
    "\n",
    "    y_val = df_val[y_label].to_numpy()\n",
    "    X_val = df_val.drop(columns=y_label).to_numpy()\n",
    "\n",
    "    y_test = df_test[y_label].to_numpy()\n",
    "    X_test = df_test.drop(columns=y_label).to_numpy()\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n"
   ],
   "id": "ba7e4b1f6f66537d",
   "outputs": [],
   "execution_count": 321
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The decision tree is built using cross entropy as the quality measure and maximum depth 10 as the stopping criterium. The tree is then pruned.",
   "id": "11e3df307a2668ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-12T00:04:10.180070Z",
     "start_time": "2025-06-12T00:04:09.491833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Split the dataset into test, train and validation sets\n",
    "Xtrain_car, ytrain_car, X_val_car, y_val_car, X_test_car, y_test_car = train_test_val(df_car,5)\n",
    "\n",
    "model = DecisionTreeClassifier(\"entropy\", max_depth=10)\n",
    "model.fit(Xtrain_car, ytrain_car)\n",
    "\n",
    "#Pruning using validation set\n",
    "prune(model.root, X_val_car, y_val_car, model)\n",
    "\n",
    "print(\"ROOT\")\n",
    "model.print_tree(model.root)\n",
    "\n",
    "# Calculate performance of the pruned tree on the test set\n",
    "y_pred = model.predict(X_test_car)\n",
    "\n",
    "loss = cross_entropy_loss(y_test_car, y_pred)\n",
    "true_loss = true_cross_entropy_loss(y_test_car, model.predict_probs(X_test_car))\n",
    "acc = accuracy(y_test_car, y_pred)\n",
    "\n",
    "print(f\"\\nCross Entropy Loss using Hard Labels (Post-Pruning): {loss:.4f}\")\n",
    "print(f\"Cross Entropy Loss using Probabilities (Post-Pruning): {true_loss:.4f}\")\n",
    "print(f\"Accuracy on Test-Set (Post-Pruning): {acc * 100:.2f}%\")"
   ],
   "id": "3ccaa1c17a931d12",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT\n",
      "└── X[5], Gain: 0.2830, Depth: 0, Entropy: 1.5838, Probs: {high: 0.34, low: 0.34, med: 0.31}\n",
      "    ├── X[5] = acc\n",
      "    │   └── X[4], Gain: 0.0209, Depth: 1, Entropy: 0.9683, Probs: {high: 0.60, med: 0.40}\n",
      "    │       ├── X[4] = big\n",
      "    │       │   └── [Leaf] Depth: 2, Class: med, Entropy: 0.9996, Probs: {high: 0.49, med: 0.51}\n",
      "    │       ├── X[4] = med\n",
      "    │       │   └── X[2], Gain: 0.0700, Depth: 2, Entropy: 0.9389, Probs: {high: 0.64, med: 0.36}\n",
      "    │       │       ├── X[2] = 2\n",
      "    │       │       │   └── X[0], Gain: 0.2365, Depth: 3, Entropy: 0.7219, Probs: {high: 0.80, med: 0.20}\n",
      "    │       │       │       ├── X[0] = high\n",
      "    │       │       │       │   └── [Leaf] Depth: 4, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │       │       │       ├── X[0] = low\n",
      "    │       │       │       │   └── [Leaf] Depth: 4, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │       │       │       ├── X[0] = med\n",
      "    │       │       │       │   └── X[1], Gain: 0.4200, Depth: 4, Entropy: 0.9710, Probs: {high: 0.60, med: 0.40}\n",
      "    │       │       │       │       ├── X[1] = high\n",
      "    │       │       │       │       │   └── [Leaf] Depth: 5, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │       │       │       │       ├── X[1] = low\n",
      "    │       │       │       │       │   └── [Leaf] Depth: 5, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │       │       │       │       └── X[1] = med\n",
      "    │       │       │       │           └── [Leaf] Depth: 5, Class: high, Entropy: 0.9183, Probs: {high: 0.67, med: 0.33}\n",
      "    │       │       │       └── X[0] = vhigh\n",
      "    │       │       │           └── [Leaf] Depth: 4, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │       │       ├── X[2] = 3\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.9710, Probs: {high: 0.60, med: 0.40}\n",
      "    │       │       ├── X[2] = 4\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Entropy: 0.9968, Probs: {med: 0.53, high: 0.47}\n",
      "    │       │       └── X[2] = 5more\n",
      "    │       │           └── [Leaf] Depth: 3, Class: high, Entropy: 0.7219, Probs: {med: 0.20, high: 0.80}\n",
      "    │       └── X[4] = small\n",
      "    │           └── X[1], Gain: 0.2662, Depth: 2, Entropy: 0.9024, Probs: {high: 0.68, med: 0.32}\n",
      "    │               ├── X[1] = high\n",
      "    │               │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.3534, Probs: {high: 0.93, med: 0.07}\n",
      "    │               ├── X[1] = low\n",
      "    │               │   └── X[0], Gain: 0.9940, Depth: 3, Entropy: 0.9940, Probs: {med: 0.55, high: 0.45}\n",
      "    │               │       ├── X[0] = high\n",
      "    │               │       │   └── [Leaf] Depth: 4, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │               │       ├── X[0] = low\n",
      "    │               │       │   └── [Leaf] Depth: 4, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │               │       ├── X[0] = med\n",
      "    │               │       │   └── [Leaf] Depth: 4, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │               │       └── X[0] = vhigh\n",
      "    │               │           └── [Leaf] Depth: 4, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │               ├── X[1] = med\n",
      "    │               │   └── X[0], Gain: 0.4764, Depth: 3, Entropy: 0.9799, Probs: {med: 0.58, high: 0.42}\n",
      "    │               │       ├── X[0] = high\n",
      "    │               │       │   └── [Leaf] Depth: 4, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │               │       ├── X[0] = low\n",
      "    │               │       │   └── [Leaf] Depth: 4, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │               │       ├── X[0] = med\n",
      "    │               │       │   └── X[2], Gain: 0.1839, Depth: 4, Entropy: 0.8631, Probs: {high: 0.29, med: 0.71}\n",
      "    │               │       │       ├── X[2] = 2\n",
      "    │               │       │       │   └── [Leaf] Depth: 5, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │               │       │       ├── X[2] = 3\n",
      "    │               │       │       │   └── [Leaf] Depth: 5, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │               │       │       ├── X[2] = 4\n",
      "    │               │       │       │   └── [Leaf] Depth: 5, Class: high, Entropy: 1.0000, Probs: {high: 0.50, med: 0.50}\n",
      "    │               │       │       └── X[2] = 5more\n",
      "    │               │       │           └── [Leaf] Depth: 5, Class: med, Entropy: 0.9183, Probs: {med: 0.67, high: 0.33}\n",
      "    │               │       └── X[0] = vhigh\n",
      "    │               │           └── [Leaf] Depth: 4, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │               └── X[1] = vhigh\n",
      "    │                   └── [Leaf] Depth: 3, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    ├── X[5] = good\n",
      "    │   └── X[4], Gain: 0.6063, Depth: 1, Entropy: 0.9656, Probs: {med: 0.39, high: 0.61}\n",
      "    │       ├── X[4] = big\n",
      "    │       │   └── [Leaf] Depth: 2, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │       ├── X[4] = med\n",
      "    │       │   └── X[2], Gain: 0.9183, Depth: 2, Entropy: 0.9183, Probs: {med: 0.33, high: 0.67}\n",
      "    │       │       ├── X[2] = 2\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │       │       ├── X[2] = 3\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    │       │       ├── X[2] = 4\n",
      "    │       │       │   └── [Leaf] Depth: 3, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │       │       └── X[2] = 5more\n",
      "    │       │           └── [Leaf] Depth: 3, Class: med, Entropy: 0.0000, Probs: {med: 1.00}\n",
      "    │       └── X[4] = small\n",
      "    │           └── [Leaf] Depth: 2, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "    ├── X[5] = unacc\n",
      "    │   └── [Leaf] Depth: 1, Class: low, Entropy: 1.4949, Probs: {high: 0.21, low: 0.49, med: 0.30}\n",
      "    └── X[5] = vgood\n",
      "        └── [Leaf] Depth: 1, Class: high, Entropy: 0.0000, Probs: {high: 1.00}\n",
      "\n",
      "Cross Entropy Loss using Hard Labels (Post-Pruning): 9.9617\n",
      "Cross Entropy Loss using Probabilities (Post-Pruning): 1.1145\n",
      "Accuracy on Test-Set (Post-Pruning): 51.93%\n"
     ]
    }
   ],
   "execution_count": 322
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Compared to the accuracy of the decision tree built wihtout pruning, the pruned tree performs consistently better.",
   "id": "24d15e9426bace3f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
